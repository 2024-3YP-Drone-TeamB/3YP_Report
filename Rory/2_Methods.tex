

    \subsubsection{System Architecture} \label{compvis_methods}
    
        \begin{enumerate}
        
            \item \textbf{Mission Planning} The user operator will select a region that they wish to map, by selecting the perimeter of the region of interest (Section \ref{sec:missionplanning}). The base-station computer will then decide on the optimal path a drone should take to get there, and cover the region of interest with the thermal sensor. A human operator then manually loads these flight instructions from the computer onto the thermal drone, after some pre-flight checks.
            
            \item \textbf{Thermal Data Acquisition} The thermal drone autonomously captures images at 1~Hz along the planned trajectory. Post-mission, images are transferred to the base station and processed by a YOLOv11 CNN, described in Section \ref{computervision}. This generates a thermal probability map \(\mathcal{P}_{\text{thermal}}(x,y)\), where each coordinate \((x,y)\) is assigned a confidence score (between 0 and 1) for landmine presence. Sparse map assembly described in Section \ref{orthomosaic} uses position and pose metadata to project detections onto a geographic grid, avoiding computationally intensive image mosaicking.
            
            \item \textbf{Radar Data Acquisition} Regions with \(\mathcal{P}_{\text{thermal}} > \mathcal{P}_{\text{threshold}}\) are flagged as \textit{Points of Interest} (POIs). A radar drone autonomously surveys these POIs, collecting GPSAR Bscans along linear transects (Section \ref{sec:msp_layered_approach}). A second YOLOv11 computer vision model (described in Section \ref{compvis_implementation} processes these Bscans to produce a radar probability map \(\mathcal{P}_{\text{radar}}(x,y)\).
            
            \item \textbf{Fusion} These two probability maps, from different sensors, but over the same geographical space, are passed into a fusion algorithm, along with some environmental contextual data, returning a third, fused, probability map $\mathcal{P}_{\text{fused}}$, that is the key output of the Section \ref{fusion}. This map can be visualised in a human-computer interface to provide actionable intelligence for demining teams
            
        \end{enumerate}


    
    \subsubsection{Summary and Key Novelty}


    
        \noindent In summary, the literature indicates that the combination of a CNN-based YOLO model, with a physics-informed deep learning fusion framework is the optimal solution for achieving high accuracy and robustness in landmine detection systems.

        \noindent The hierarchical sensor integration described ensures much faster detection, with minimal compromise on accuracy, by leveraging the high recall, high speed thermal sensors, and the slower, high precision radar sensor a very efficient manner. This 'layered approach' is justified from a flight efficiency standpoint in Section \ref{layered_approach}, and a computer vision standpoint in Section \ref{fusion}.
            
        \noindent The system outlined in this report represents a novelty in the fusion approach. The layered approach for landmine detection has not yet been justified from a computer vision standpoint before, and the specific physics-informed deep learning model for late stage fusion used in Section \ref{fusion}, has not been used before in the context of landmine detection.

        The next section, \ref{compvis_implementation}, elaborates on sensor-specific implementations, where the precision and recall of the thermal and radar subsystems will be quantified.


            
    %\subsubsection{Time Efficiency Analysis} 
    %Let \(A\) represent the total survey area, \(t_T\) the time per unit area for thermal scanning, and \(t_R\) the time per unit area for radar scanning. The total time for layered scanning comprises two components: thermal coverage of the entire area and radar scrutiny of high-probability regions:
    
    %\begin{equation}
        %t_{\text{layered}} = \underbrace{t_TA}_{\text{Thermal}} + \underbrace{t_R A (R_T \rho_0 + F_T (1 - \rho_0))}_{\text{Radar on } \mathcal{T}^+}.
    %\end{equation}
    
    %For representative parameters (\(R_T = 0.95\), \(F_T = 0.05\), \(\rho_0 = 0.05\), \(t_T = 0.1t_R\)):
    
    %\begin{equation}
        %t_{\text{layered}} = 0.1t_R A + t_R A \left((0.95 \times 0.05) + (0.05 \times 0.95)\right) =  0.195t_R A.
    %\end{equation}
    
    %A simultaneous full-coverage deployment, requiring both sensors to scan the entire area independently, would demand:
    
    %\begin{equation}
        %t_{\text{parallel}} = \max(t_T, t_R) %\cdot A = t_R A,
    %\end{equation}
    
    %as the radar's slower scan rate (\(t_R > t_T\)) dictates operational tempo. The layered approach therefore achieves an \textbf{80.5\% reduction} in operational time compared to exhaustive parallel scanning with these representative parameters.
    
    %Continuing this example, $\rho_1 \approx \text{50\%}$, meaning that the density of mines in the flagged regions was ten times that of the original area. This underscores the reasoning behind only using the radar sensors in the flagged, mine rich regions.
